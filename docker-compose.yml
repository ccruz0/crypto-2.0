name: automated-trading-platform

services:
  # ============================================
  # GLUETUN (VPN Container) - REMOVED
  # ============================================
  # Gluetun has been removed as the system now uses direct AWS Elastic IP connection
  # Backend connects directly to Crypto.com Exchange via AWS Elastic IP 47.130.143.159
  # No VPN is needed. See MIGRATION_TO_DIRECT_AWS_IP_REPORT.md for details.
  #
  # gluetun:
  #   image: qmcgaw/gluetun:latest
  #   container_name: gluetun
  #   ... (removed - not needed)

  # ============================================
  # DATABASE - Both Profiles
  # ============================================
  db:
    build:
      context: ./docker/postgres
      dockerfile: Dockerfile
    container_name: postgres_hardened
    env_file:
      - .env
      - .env.local
      - .env.aws
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-atp}
      POSTGRES_USER: ${POSTGRES_USER:-trader}
      # POSTGRES_PASSWORD from env_file only (.env / .env.aws) - never inline
      POSTGRES_INITDB_ARGS: --auth=scram-sha-256
    volumes:
      - postgres_data:/var/lib/postgresql/data
    # SECURITY: No ports mapping - database is only accessible via Docker network
    # Backend connects using service name: postgresql://trader:pass@db:5432/atp
    # Verify: ss -lntp | grep 5432 should return empty (no public listener)
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: always
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:-trader}"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles:
      - local
      - aws

  # ============================================
  # BACKEND-DEV - Local Profile (Hot Reload Development)
  # ============================================
  # Fast hot-reload development service for local development.
  # Mounts only app code (preserves installed packages in image).
  # Uses uvicorn --reload for automatic code reloading.
  # ⚠️ WARNING: This is for LOCAL DEVELOPMENT ONLY
  # Do NOT run this in parallel with AWS production backend.
  backend-dev:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: automated-trading-platform-backend-dev
    env_file:
      - .env
      - .env.local
      # NOTE: .env.secrets.local is optional - if you need it, create it manually and it will be auto-loaded
      # NOTE: .env.aws is NOT loaded for local dev to avoid AWS config conflicts
    environment:
      # DATABASE_URL from env_file only
      - ENVIRONMENT=local
      - APP_ENV=local
      - RUNTIME_ORIGIN=LOCAL
      - TRADING_ENABLED=false
      - LIVE_TRADING=false
      - RUN_TELEGRAM=false
      - USE_CRYPTO_PROXY=false
      - API_BASE_URL=http://localhost:8002
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3001}
      - DISABLE_AUTH=true
      - ENABLE_CORS=1
      - PYTHONUNBUFFERED=1
      - DEBUG_DASHBOARD=1
    ports:
      - "8002:8002"
    volumes:
      # Mount only app code for hot-reload (preserves installed packages in image)
      - ./backend/app:/app/app
      - ./backend/scripts:/app/scripts
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: python -m uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload --log-level info --access-log --timeout-keep-alive 30
    restart: always
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;urllib.request.urlopen('http://localhost:8002/api/health', timeout=3)"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "768M"
    depends_on:
      db:
        condition: service_healthy
    profiles:
      - local

  # ============================================
  # BACKEND - Local Profile (Direct Connection)
  # ============================================
  # ⚠️ WARNING: This is for LOCAL DEVELOPMENT ONLY
  # Do NOT run this in parallel with AWS production backend.
  # AWS backend is the ONLY live production runtime (trading + alerts).
  # This local profile is for development/testing only.
  # NOTE: For hot-reload development, use backend-dev instead.
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
      - .env.local
      - .env.aws
    environment:
      # DATABASE_URL, TELEGRAM_BOT_TOKEN_LOCAL, TELEGRAM_CHAT_ID_LOCAL, CRYPTO_PROXY_TOKEN from env_file only
      - ENVIRONMENT=${ENVIRONMENT:-local}
      - APP_ENV=${APP_ENV:-local}
      - RUN_TELEGRAM=${RUN_TELEGRAM:-true}
      - RUNTIME_ORIGIN=${RUNTIME_ORIGIN:-LOCAL}
      - TRADING_ENABLED=${TRADING_ENABLED:-false}
      - EXCHANGE_CUSTOM_BASE_URL=https://api.crypto.com/exchange/v1
      - CRYPTO_REST_BASE=https://api.crypto.com/exchange/v1
      - API_BASE_URL=${API_BASE_URL:-http://localhost:8002}
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3001}
      - LIVE_TRADING=${LIVE_TRADING:-true}
      - USE_CRYPTO_PROXY=${USE_CRYPTO_PROXY:-true}
      - CRYPTO_PROXY_URL=${CRYPTO_PROXY_URL:-http://host.docker.internal:9000}
      - UVICORN_WORKERS=1
      - DISABLE_MIDDLEWARES=0
      - ENABLE_CORS=1
      - DISABLE_AUTH=${DISABLE_AUTH:-true}
      - VPN_GATE_ENABLED=${VPN_GATE_ENABLED:-true}
      - VPN_GATE_URL=${VPN_GATE_URL:-https://api.crypto.com/v2/public/get-ticker?instrument_name=BTC_USDT}
      - VPN_GATE_EXPECTS_JSON=${VPN_GATE_EXPECTS_JSON:-true}
      - VPN_GATE_TIMEOUT_SECS=${VPN_GATE_TIMEOUT_SECS:-3}
      - VPN_GATE_RETRY_SECS=${VPN_GATE_RETRY_SECS:-5}
      - VPN_GATE_MAX_WAIT_SECS=${VPN_GATE_MAX_WAIT_SECS:-120}
      - VPN_GATE_BACKGROUND=${VPN_GATE_BACKGROUND:-true}
      - VPN_GATE_DEV_MAX_WAIT_SECS=${VPN_GATE_DEV_MAX_WAIT_SECS:-15}
      # Portfolio debug (enables diagnostics endpoints)
      - PORTFOLIO_DEBUG=${PORTFOLIO_DEBUG:-1}
      # Portfolio reconcile debug (enables reconcile bundle in portfolio response)
      - PORTFOLIO_RECONCILE_DEBUG=${PORTFOLIO_RECONCILE_DEBUG:-1}
    ports:
      - "8002:8002"
    volumes:
      - ./backend:/app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: sh -c "sleep 10 && python -m uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload --log-level info --access-log --timeout-keep-alive 30 --timeout-graceful-shutdown 10"
    restart: always
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;urllib.request.urlopen('http://localhost:8002/ping_fast', timeout=3)"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "768M"
    depends_on:
      db:
        condition: service_healthy
    profiles:
      - local

  # ============================================
  # BACKEND - AWS Profile (Direct Connection)
  # ============================================
  # AWS alert/Telegram: ENVIRONMENT=aws, RUNTIME_ORIGIN=AWS, RUN_TELEGRAM=true (injected below).
  # Telegram credentials from .env.aws / secrets/runtime.env (TELEGRAM_BOT_TOKEN_AWS, TELEGRAM_CHAT_ID_AWS).
  backend-aws:
    image: automated-trading-platform-backend-aws
    build:
      context: .
      dockerfile: backend/Dockerfile.aws
      args:
        GIT_SHA: ${GIT_SHA:-unknown}
        BUILD_TIME: ${BUILD_TIME:-unknown}
    env_file:
      - .env
      - .env.aws
      - ./secrets/runtime.env
      # CRITICAL: Do NOT load .env.local - AWS must never have LOCAL Telegram credentials
      # This ensures AWS backend cannot access TELEGRAM_BOT_TOKEN_LOCAL or TELEGRAM_CHAT_ID_LOCAL
    environment:
      # DATABASE_URL from env_file only; host must be "db" (not a container IP) or backend gets 503
      - ENVIRONMENT=aws
      - APP_ENV=aws
      - RUN_TELEGRAM=${RUN_TELEGRAM:-true}
      - RUNTIME_ORIGIN=AWS
      # Required for Crypto.com private API: allow get_account_summary / user-balance (portfolio)
      - EXECUTION_CONTEXT=AWS
      - TRADING_ENABLED=${TRADING_ENABLED:-true}
      # Health monitoring thresholds
      - HEALTH_STALE_MARKET_MINUTES=${HEALTH_STALE_MARKET_MINUTES:-30}
      - HEALTH_MONITOR_STALE_MINUTES=${HEALTH_MONITOR_STALE_MINUTES:-30}
      - SYSTEM_ALERT_COOLDOWN_HOURS=${SYSTEM_ALERT_COOLDOWN_HOURS:-24}
      - EXCHANGE_CUSTOM_BASE_URL=https://api.crypto.com/exchange/v1
      - CRYPTO_REST_BASE=https://api.crypto.com/exchange/v1
      # Force internal API URL for Docker service-to-service communication
      - API_BASE_URL=http://backend-aws:8002
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3001}
      - LIVE_TRADING=${LIVE_TRADING:-true}
      # Build fingerprint env vars (middleware reads ATP_GIT_SHA and ATP_BUILD_TIME)
      # These are set from build args during image build, but also set here as runtime override
      - ATP_GIT_SHA=${ATP_GIT_SHA:-${GIT_SHA:-unknown}}
      - ATP_BUILD_TIME=${ATP_BUILD_TIME:-${BUILD_TIME:-unknown}}
      # Direct connection to Crypto.com Exchange via AWS Elastic IP (no proxy, no VPN)
      # Backend connects directly from AWS Elastic IP 47.130.143.159 to api.crypto.com/exchange/v1
      - USE_CRYPTO_PROXY=${USE_CRYPTO_PROXY:-false}
      - CRYPTO_PROXY_URL=${CRYPTO_PROXY_URL:-http://host.docker.internal:9000}
      # CRYPTO_PROXY_TOKEN from env_file only
      - UVICORN_WORKERS=1
      - DISABLE_MIDDLEWARES=0
      - ENABLE_CORS=1
      - DISABLE_AUTH=${DISABLE_AUTH:-false}
      - VPN_GATE_ENABLED=${VPN_GATE_ENABLED:-true}
      - VPN_GATE_URL=${VPN_GATE_URL:-https://api.crypto.com/v2/public/get-ticker?instrument_name=BTC_USDT}
      - VPN_GATE_EXPECTS_JSON=${VPN_GATE_EXPECTS_JSON:-true}
      - VPN_GATE_TIMEOUT_SECS=${VPN_GATE_TIMEOUT_SECS:-3}
      - VPN_GATE_RETRY_SECS=${VPN_GATE_RETRY_SECS:-5}
      - VPN_GATE_MAX_WAIT_SECS=${VPN_GATE_MAX_WAIT_SECS:-120}
      - VPN_GATE_BACKGROUND=${VPN_GATE_BACKGROUND:-true}
      - VPN_GATE_DEV_MAX_WAIT_SECS=${VPN_GATE_DEV_MAX_WAIT_SECS:-15}
      - ENABLE_TEST_PRICE_INJECTION=${ENABLE_TEST_PRICE_INJECTION:-0}
      # Diagnostics: enable so OpenAPI and routes are active; key from env_file (secrets/runtime.env)
      - ENABLE_DIAGNOSTICS_ENDPOINTS=1
      # DIAGNOSTICS_API_KEY from env_file only (secrets/runtime.env) - never commit
      # Price move alert configuration
      - PRICE_MOVE_ALERT_PCT=${PRICE_MOVE_ALERT_PCT:-0.50}
      - PRICE_MOVE_ALERT_COOLDOWN_SECONDS=${PRICE_MOVE_ALERT_COOLDOWN_SECONDS:-300}
      # Debug: duplicate request guard for signals endpoint (disabled by default, enable via .env.aws if needed)
      - SIGNALS_DUP_GUARD=${SIGNALS_DUP_GUARD:-0}
      # Portfolio reconcile debug (enables reconcile bundle in portfolio response)
      - PORTFOLIO_RECONCILE_DEBUG=${PORTFOLIO_RECONCILE_DEBUG:-1}
      # Portfolio debug (enables diagnostics endpoints like /api/diagnostics/portfolio/reconcile and /api/diagnostics/whoami)
      # Can be overridden via .env.aws (loaded via env_file above). Defaults to 1 if not set.
      # Single source of truth: docker-compose.yml environment section (can be overridden by .env.aws)
      - PORTFOLIO_DEBUG=${PORTFOLIO_DEBUG:-1}
      # Trading limits: MAX_OPEN_ORDERS_TOTAL controls max pending TP orders (new TP-only counting logic)
      - MAX_OPEN_ORDERS_TOTAL=${MAX_OPEN_ORDERS_TOTAL:-10}
      # Telegram/admin keys are injected via secrets/runtime.env (rendered by scripts/aws/render_runtime_env.sh)
      # Persist strategy config so dashboard strategy changes survive restarts (RUNBOOK 8.5)
      - TRADING_CONFIG_PATH=/data/trading_config.json
      # Option A: tail_logs + host-visible ai_runs
      - AI_ENGINE_COMPOSE_DIR=/app
      - AI_RUNS_DIR=/app/backend/ai_runs
    # SECURITY NOTE:
    # Never publish backend ports on 0.0.0.0 in production.
    # Use 127.0.0.1 binding or internal docker network only.
    # Public access must go through nginx only.
    ports:
      - "127.0.0.1:8002:8002"
    # volumes:
    #   - ./backend:/app  # Commented out for production - use built image instead of mounting local files
    volumes:
      - ./docker-compose.yml:/app/docker-compose.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./backend/ai_runs:/app/backend/ai_runs
      - aws_trading_config_data:/data
    group_add:
      - "0"
    working_dir: /app
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # Prod must not use --reload; it causes restarts and 502s
    # Use >1 worker so /ping_fast stays responsive even during slow endpoints.
    command: sh -c "sleep 10 && python -m gunicorn app.main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8002 --log-level info --access-logfile - --timeout 120 --max-requests 10000 --max-requests-jitter 50"
    restart: always
    # Healthcheck: Python one-liner to avoid bash/shell forks (containerd-shim zombie leak)
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8002/ping_fast', timeout=5)"]
      interval: 120s
      timeout: 20s
      retries: 5
      start_period: 180s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "1G"
    depends_on:
      db:
        condition: service_healthy
    profiles:
      - aws

  # ============================================
  # BACKEND AWS CANARY (same image as backend-aws, port 8003; for weighted deploy)
  # ============================================
  backend-aws-canary:
    image: automated-trading-platform-backend-aws
    env_file:
      - .env
      - .env.aws
      - ./secrets/runtime.env
    environment:
      # DATABASE_URL from env_file only
      - ENVIRONMENT=aws
      - APP_ENV=aws
      - RUN_TELEGRAM=${RUN_TELEGRAM:-true}
      - RUNTIME_ORIGIN=AWS
      - EXECUTION_CONTEXT=AWS
      - TRADING_ENABLED=${TRADING_ENABLED:-true}
      - EXCHANGE_CUSTOM_BASE_URL=https://api.crypto.com/exchange/v1
      - CRYPTO_REST_BASE=https://api.crypto.com/exchange/v1
      - API_BASE_URL=http://backend-aws-canary:8002
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3001}
      - LIVE_TRADING=${LIVE_TRADING:-true}
      - USE_CRYPTO_PROXY=${USE_CRYPTO_PROXY:-false}
      - UVICORN_WORKERS=1
      - ENABLE_DIAGNOSTICS_ENDPOINTS=1
      - PORTFOLIO_DEBUG=${PORTFOLIO_DEBUG:-1}
      - PORTFOLIO_RECONCILE_DEBUG=${PORTFOLIO_RECONCILE_DEBUG:-1}
      - TRADING_CONFIG_PATH=/data/trading_config.json
    # Bind to loopback only (Phase F: no public binds for 8002/3000/8003)
    ports:
      - "127.0.0.1:8003:8002"
    volumes:
      - aws_trading_config_data:/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: sh -c "sleep 10 && python -m gunicorn app.main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8002 --log-level info --access-logfile - --timeout 120 --max-requests 10000 --max-requests-jitter 50"
    restart: "no"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;urllib.request.urlopen('http://localhost:8002/ping_fast', timeout=10)"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "1G"
    depends_on:
      db:
        condition: service_healthy
    profiles:
      - aws

  # ============================================
  # MARKET UPDATER - Local Profile
  # ============================================
  market-updater:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
      - .env.local
    environment:
      # DATABASE_URL, CRYPTO_PROXY_TOKEN from env_file only
      - ENVIRONMENT=${ENVIRONMENT:-local}
      - APP_ENV=${APP_ENV:-local}
      - RUN_TELEGRAM=${RUN_TELEGRAM:-false}
      - RUNTIME_ORIGIN=LOCAL
      - LIVE_TRADING=${LIVE_TRADING:-true}
      - USE_CRYPTO_PROXY=${USE_CRYPTO_PROXY:-true}
      - CRYPTO_PROXY_URL=${CRYPTO_PROXY_URL:-http://host.docker.internal:9000}
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./backend:/app
    command: python3 run_updater.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;urllib.request.urlopen('http://backend:8002/ping_fast', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - local

  # ============================================
  # MARKET UPDATER - AWS Profile
  # ============================================
  # Alerts are generated here (signal_monitor). Same AWS env as backend-aws so Telegram sends.
  # Do NOT load .env.local - AWS must use only .env + .env.aws (Telegram: TELEGRAM_BOT_TOKEN_AWS, TELEGRAM_CHAT_ID_AWS).
  market-updater-aws:
    build:
      context: ./backend
      dockerfile: Dockerfile
    env_file:
      - .env
      - .env.aws
      - ./secrets/runtime.env
    environment:
      # DATABASE_URL, CRYPTO_PROXY_TOKEN from env_file only
      - ENVIRONMENT=aws
      - APP_ENV=aws
      - RUN_TELEGRAM=${RUN_TELEGRAM:-true}
      - RUNTIME_ORIGIN=AWS
      - LIVE_TRADING=${LIVE_TRADING:-true}
      - USE_CRYPTO_PROXY=${USE_CRYPTO_PROXY:-true}
      - CRYPTO_PROXY_URL=${CRYPTO_PROXY_URL:-http://host.docker.internal:9000}
      - EXCHANGE_CUSTOM_BASE_URL=https://api.crypto.com/exchange/v1
      - CRYPTO_REST_BASE=https://api.crypto.com/exchange/v1
      # Price move alert configuration (market-updater runs signal monitor loop)
      - PRICE_MOVE_ALERT_PCT=${PRICE_MOVE_ALERT_PCT:-0.50}
      - PRICE_MOVE_ALERT_COOLDOWN_SECONDS=${PRICE_MOVE_ALERT_COOLDOWN_SECONDS:-300}
      # TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID are loaded from .env.aws via env_file
    depends_on:
      db:
        condition: service_healthy
    # volumes:
    #   - ./backend:/app  # Commented out for production - use built image instead of mounting local files
    command: python3 run_updater.py
    restart: unless-stopped
    # Healthcheck: Python one-liner to avoid bash/shell forks (containerd-shim zombie leak)
    healthcheck:
      test: ["CMD", "python", "-c", "print('ok')"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 30s
    profiles:
      - aws

  # ============================================
  # FRONTEND - Local Profile (Direct Connection)
  # ============================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    env_file:
      - .env
      - .env.local
      - .env.aws
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - NEXT_PUBLIC_API_URL=http://localhost:8002/api
      - NEXT_PUBLIC_ENVIRONMENT=${NEXT_PUBLIC_ENVIRONMENT:-local}
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    command: npm run dev
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp
    restart: always
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/"]
      interval: 30s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "512M"
    depends_on:
      backend:
        condition: service_healthy
    profiles:
      - local

  # ============================================
  # FRONTEND - AWS Profile
  # ============================================
  frontend-aws:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    env_file:
      - .env
      - .env.local
      - .env.aws
    environment:
      - NODE_ENV=production
      # Use relative path for client-side - environment.ts will detect hostname at runtime
      # Empty string can cause issues, so use relative path which works in both SSR and browser
      - NEXT_PUBLIC_API_URL=/api
      - NEXT_PUBLIC_ENVIRONMENT=${NEXT_PUBLIC_ENVIRONMENT:-aws}
    ports:
      - "127.0.0.1:3000:3000"
    # Production build: use CMD from Dockerfile (node server.js)
    # For faster updates during development, you can temporarily mount volumes:
    # volumes:
    #   - ./frontend:/app
    #   - /app/node_modules
    #   - /app/.next
    # And use: command: npm run dev
    # But for production, keep read_only and no volumes for security
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp
    restart: always
    healthcheck:
      test: ["CMD", "sh", "-c", "exit 0"]
      interval: 30s
      timeout: 5s
      retries: 1
      start_period: 5s
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "512M"
    depends_on:
      backend-aws:
        condition: service_healthy
      # Gluetun dependency removed - frontend doesn't need VPN
      # gluetun:
      #   condition: service_healthy
    profiles:
      - aws

  # ============================================
  # OBSERVABILITY (internal only; bind 127.0.0.1)
  # ============================================
  prometheus:
    image: prom/prometheus:v2.51.2
    container_name: atp-prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
    volumes:
      - ./scripts/aws/observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./scripts/aws/observability/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "127.0.0.1:9090:9090"
    depends_on:
      - cadvisor
      - node-exporter
      - backend-aws
      - market-updater-aws
      - alertmanager
    restart: unless-stopped
    profiles:
      - aws

  grafana:
    image: grafana/grafana:10.4.1
    container_name: atp-grafana
    env_file:
      - .env
      - .env.aws
      - ./secrets/runtime.env
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_HTTP_PORT=3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./scripts/aws/observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./scripts/aws/observability/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "127.0.0.1:3001:3000"
    depends_on:
      - prometheus
    restart: unless-stopped
    profiles:
      - aws

  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: atp-alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
    volumes:
      - ./scripts/aws/observability/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    ports:
      - "127.0.0.1:9093:9093"
    depends_on:
      - telegram-alerts
    restart: unless-stopped
    profiles:
      - aws

  telegram-alerts:
    build:
      context: ./scripts/aws/observability/telegram-alerts
    container_name: atp-telegram-alerts
    env_file:
      - .env
      - .env.aws
      - ./secrets/runtime.env
    restart: unless-stopped
    profiles:
      - aws

  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: atp-node-exporter
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/rootfs
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "127.0.0.1:9100:9100"
    restart: unless-stopped
    profiles:
      - aws

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: atp-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "127.0.0.1:8080:8080"
    restart: unless-stopped
    profiles:
      - aws

  # ============================================
  # AWS-SPECIFIC SERVICES
  # ============================================
  aws-backup:
    build:
      context: ./docker/postgres
      dockerfile: Dockerfile
    container_name: postgres_hardened_backup
    env_file:
      - .env
      - .env.aws
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-atp}
      POSTGRES_USER: ${POSTGRES_USER:-trader}
      # POSTGRES_PASSWORD from env_file only
    volumes:
      - aws_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-trader} -d ${POSTGRES_DB:-atp}"]
      interval: 30s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    profiles:
      - aws

volumes:
  postgres_data:
  aws_postgres_data:
  prometheus_data:
  grafana_data:
  # For RUNBOOK 8.5: persist strategy config; uncomment backend-aws volume mount and TRADING_CONFIG_PATH to use
  aws_trading_config_data:

secrets:
  pg_password:
    file: ./secrets/pg_password
